{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance\n",
    "from ui_script import show_top_5\n",
    "import pickle\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v = Word2Vec.load_word2vec_format('../../../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec_dict = np.load('image_space.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a query image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random select query image and caption\n",
    "query_file_name = random.choice(list(new_vec_dict.keys()))\n",
    "query_vec = new_vec_dict[query_file_name]\n",
    "query_caption = w2v.most_similar(query_vec, restrict_vocab=30000, topn=1)[0][0]\n",
    "\n",
    "# top 5 images\n",
    "top5_file_names = ['a', 'a', 'a', 'a', 'a']\n",
    "top5_captions = ['a', 'a', 'a', 'a', 'a']\n",
    "top5_distances = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "# Iterate over all images\n",
    "for file_name in new_vec_dict.keys():\n",
    "    if file_name != query_file_name:\n",
    "        \n",
    "#        # Iterate over all captions belonging to an image\n",
    "#        closest_distance = 1.0\n",
    "#        idx = -1\n",
    "#        for i, w2v in enumerate(w2v_dic[file_name]):\n",
    "#            dis = scipy.spatial.distance.cosine(query_w2v,w2v)\n",
    "#            if dis < closest_distance:\n",
    "#                closest_distance = dis\n",
    "#                idx = i\n",
    "\n",
    "        # See if closest caption is in top 5\n",
    "        for i, dist in enumerate(top5_distances):\n",
    "            temp_dist = scipy.spatial.distance.cosine(query_vec,new_vec_dict[file_name])\n",
    "            if temp_dist < dist:\n",
    "                top5_distances[i] = temp_dist\n",
    "#                top5_captions[i] = caption_dic[file_name][idx] + \"; \" + str(round(closest_distance, 4))\n",
    "                top5_file_names[i] = '../../../data/train2014/' + file_name\n",
    "#                top5_idx[i] = idx\n",
    "                break\n",
    "\n",
    "# Find most similar word in word2vec\n",
    "for i in range(5):\n",
    "    file_name = re.search('COCO.+',top5_file_names[i])[0]\n",
    "    top5_captions[i] =w2v.most_similar(\n",
    "        np.mean([query_vec,new_vec_dict[file_name]],axis=0), restrict_vocab=30000, topn=1)[0][0]\n",
    "\n",
    "# Display result\n",
    "file_names = ['../../../data/train2014/' + query_file_name] + top5_file_names\n",
    "captions = [query_caption] + top5_captions\n",
    "show_top_5(file_names, captions, height = 250, title='Image space evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
