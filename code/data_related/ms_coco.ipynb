{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_sentence(sentence):\n",
    "    # Clean a sentence from unwanted characters\n",
    "    sentence = sentence.replace('.','')\n",
    "    sentence = sentence.replace(',','')\n",
    "    sentence = sentence.replace('?','')\n",
    "    sentence = sentence.replace('!','')\n",
    "    sentence = sentence.replace(';','')\n",
    "    sentence = sentence.replace(':','')\n",
    "    sentence = sentence.replace('-','')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../../data/annotations/captions_train2014.json') as data_file:  \n",
    "    captions = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Create list of sentences\n",
    "for i in range(len(captions['annotations'])):\n",
    "    sentence = captions['annotations'][i]['caption']\n",
    "    sentence = clean_sentence(sentence)\n",
    "    sentence = [word.lower() for word in sentence.split()]\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "# Train bigram\n",
    "bigram_model = gensim.models.phrases.Phrases(sentences, min_count=4, threshold=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word2vec dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "\n",
    "for i in range(len(captions['annotations'])):\n",
    "    if (i % 10000) == 0:\n",
    "        print(i)\n",
    "    # Find image filename\n",
    "    image_id = captions['annotations'][i]['image_id']\n",
    "    j, found = 0, 0\n",
    "    while found == 0:\n",
    "        if captions['images'][j]['id'] == image_id:\n",
    "            file_name = captions['images'][j]['file_name']\n",
    "            found = 1\n",
    "        j += 1\n",
    "    \n",
    "    # Create bigrams from caption\n",
    "    caption = clean_sentence(captions['annotations'][i]['caption'])\n",
    "    caption = [word.lower() for word in caption.split()]\n",
    "    caption = bigram_model[caption]\n",
    "    \n",
    "    # Remove unknown bigrams\n",
    "    temp_bigram_sentence = []\n",
    "    for j in range(len(caption)):\n",
    "        if caption[j] not in w2v_model.vocab:\n",
    "            temp_bigram_sentence += caption[j].split('_')\n",
    "        else:\n",
    "            temp_bigram_sentence.append(caption[j])\n",
    "    caption = temp_bigram_sentence\n",
    "    \n",
    "    # Remove stop words in caption\n",
    "    caption = [word for word in caption if word not in stop_words]\n",
    "    \n",
    "    # Create word2vec representation of caption\n",
    "    w2v_vector = np.zeros(300)\n",
    "    for word in caption:\n",
    "        if word in w2v_model.vocab:\n",
    "            w2v_vector += w2v_model[word]\n",
    "    \n",
    "    # Add w2v_vector to dictionary\n",
    "    temp_list = word2vec.get(file_name,[])\n",
    "    temp_list.append(w2v_vector)\n",
    "    word2vec[file_name] = temp_list\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
