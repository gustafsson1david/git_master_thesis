{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_sentence(sentence):\n",
    "    # Clean a sentence from unwanted characters\n",
    "    sentence = sentence.replace('.','')\n",
    "    sentence = sentence.replace(',','')\n",
    "    sentence = sentence.replace('?','')\n",
    "    sentence = sentence.replace('!','')\n",
    "    sentence = sentence.replace(';','')\n",
    "    sentence = sentence.replace(':','')\n",
    "    sentence = sentence.replace('-','')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "with open('../../data/annotations/captions_train2014.json') as data_file:  \n",
    "    captions_train = json.load(data_file)\n",
    "\n",
    "# Validation\n",
    "with open('../../data/annotations/captions_val2014.json') as data_file:  \n",
    "    captions_val = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Create list of sentences train\n",
    "for i in range(len(captions_train['annotations'])):\n",
    "    sentence = captions_train['annotations'][i]['caption']\n",
    "    sentence = clean_sentence(sentence)\n",
    "    sentence = [word.lower() for word in sentence.split()]\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "# Appen list of sentences validation\n",
    "for i in range(len(captions_val['annotations'])):\n",
    "    sentence = captions_val['annotations'][i]['caption']\n",
    "    sentence = clean_sentence(sentence)\n",
    "    sentence = [word.lower() for word in sentence.split()]\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "# Train bigram\n",
    "bigram_model = gensim.models.phrases.Phrases(sentences, min_count=4, threshold=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word2vec dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose train or validation\n",
    "captions = captions_train\n",
    "\n",
    "word2vec = {}\n",
    "\n",
    "for i in range(len(captions['annotations'])):\n",
    "    if (i % 10000) == 0:\n",
    "        print(i)\n",
    "    # Find image filename\n",
    "    image_id = captions['annotations'][i]['image_id']\n",
    "    j, found = 0, 0\n",
    "    while found == 0:\n",
    "        if captions['images'][j]['id'] == image_id:\n",
    "            file_name = captions['images'][j]['file_name']\n",
    "            found = 1\n",
    "        j += 1\n",
    "        \n",
    "    # Hardcoded improvements\n",
    "    caption = clean_sentence(captions['annotations'][i]['caption'])\n",
    "    caption.replace('hot dog','sausage')\n",
    "    caption.replace('hot dogs','sausages')\n",
    "    caption = [word.lower() for word in caption.split()]\n",
    "\n",
    "    \n",
    "    # Create bigrams from caption\n",
    "    caption = bigram_model[caption]\n",
    "    \n",
    "    # Remove unknown bigrams\n",
    "    temp_bigram_sentence = []\n",
    "    for j in range(len(caption)):\n",
    "        if caption[j] not in w2v_model.vocab:\n",
    "            temp_bigram_sentence += caption[j].split('_')\n",
    "        else:\n",
    "            temp_bigram_sentence.append(caption[j])\n",
    "    caption = temp_bigram_sentence\n",
    "    \n",
    "    # Remove stop words in caption\n",
    "    caption = [word for word in caption if word not in stop_words]\n",
    "    \n",
    "    # Create word2vec representation of caption\n",
    "    w2v_vector = np.zeros(300)\n",
    "    for word in caption:\n",
    "        if word in w2v_model.vocab:\n",
    "            w2v_vector += w2v_model[word]\n",
    "    \n",
    "    # Add w2v_vector to dictionary\n",
    "    temp_list = word2vec.get(file_name,[])\n",
    "    temp_list.append(w2v_vector)\n",
    "    word2vec[file_name] = temp_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose train or validation\n",
    "captions = captions_val\n",
    "\n",
    "caption_dic = {}\n",
    "\n",
    "for i in range(len(captions['annotations'])):\n",
    "    if (i % 10000) == 0:\n",
    "        print(i)\n",
    "        \n",
    "    # Find image filename\n",
    "    image_id = captions['annotations'][i]['image_id']\n",
    "    j, found = 0, 0\n",
    "    while found == 0:\n",
    "        if captions['images'][j]['id'] == image_id:\n",
    "            file_name = captions['images'][j]['file_name']\n",
    "            found = 1\n",
    "        j += 1\n",
    "    \n",
    "    # Add caption\n",
    "    temp_list = caption_dic.get(file_name, [])\n",
    "    temp_list.append(captions['annotations'][i]['caption'])\n",
    "    caption_dic[file_name] = temp_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Explanatory words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "w2v_dic = np.load('../../data/word2vec_train.npy')\n",
    "w2v_dic = w2v_dic[()]\n",
    "\n",
    "\n",
    "# Iterate over all w2v-vectors and find most similar word\n",
    "explanatory_words = {}\n",
    "for count,file_name in enumerate(w2v_dic):\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    for w2v in w2v_dic[file_name]:\n",
    "        words = w2v_model.most_similar([w2v],topn=1)\n",
    "        for word in words:\n",
    "            weight = explanatory_words.get(word[0],0)\n",
    "            explanatory_words[word[0]] = weight + word[1]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
